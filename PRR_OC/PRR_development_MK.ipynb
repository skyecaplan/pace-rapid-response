{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c5f681",
   "metadata": {},
   "source": [
    "### PACE Rapid Response Notebook - for workshopping workflow\n",
    "\n",
    "To Do:\n",
    "- adapt hackweek code to pull pace data for a given product, lat/lon extent, and time range defined before and after an event of interest\n",
    "- spatially bin l2 data consistently?\n",
    "- Make a mask of only pixels present both datasets (to prevent bias), compare data w/ pretty maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import earthaccess\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "import cmocean\n",
    "from matplotlib.colors import LogNorm\n",
    "from scipy.interpolate import griddata\n",
    "from dask.distributed import Client\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image, ImageEnhance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796357c9",
   "metadata": {},
   "source": [
    "### Development test-case\n",
    "Hurricane Erin, look at chl-a before and after hurricane for\n",
    "Timeline (using worldview): \n",
    "- Hurricane off florida/gulf stream PACE imagery on AUG 20\n",
    "- AUG 21, hurricane passed and clear imagery\n",
    "- Aug 18/19 hurricane not there yet\n",
    "\n",
    "Lat/Lon bounds: \n",
    "UL: 32.09, -76.63 \n",
    "LR: 27.78, -77.80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36b2d7",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "- use a datapoint in the center of AOI and time bounds/ earthaccess search to determine a granule form which we will define our L3M-like grid (see dask_gridding tutorial notebook)\n",
    "- define crs, tramsform, shape from this L3M-like granule (chla_L3M_aoi in dask_griddding.ipynb notebook from hackweek)\n",
    "- do another earthdata search, for full AOI over time period (1 week ish pre hurricane)\n",
    "- open all and calculate average of parameter (in this case chl-a) using dask (follow dask_griddding.ipynb section 4)\n",
    "- repeat above with timespan post hurricane, using same crs, tramsform, shape for gridding\n",
    "- at this point, have pre and post hurricane chl averages on same spatial grid. now, mask each for pixels that have date pre and post hurricane\n",
    "- make graphic showing before/after. Calculate average pre and post (using mask to avoid bias) and calculate a % increase in chl-a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112534cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newer workflow - test making the geographic parameter and dask steps functions\n",
    "\n",
    "# all functions initilized here:\n",
    "\n",
    "# Define Functions, don't modify\n",
    "def grid_match(path, dst_crs, dst_shape, dst_transform, variable):\n",
    "    \"\"\"Reproject a Level-2 granule to match a Level-3M-ish granule.\"\"\"\n",
    "    dt = xr.open_datatree(path)\n",
    "    da = dt[\"geophysical_data\"][variable]\n",
    "    da = da.rio.set_spatial_dims(\"pixels_per_line\", \"number_of_lines\")\n",
    "    da = da.rio.set_crs(\"epsg:4326\")\n",
    "    da = da.rio.reproject(\n",
    "        dst_crs,\n",
    "        shape=dst_shape,\n",
    "        transform=dst_transform,\n",
    "        src_geoloc_array=(\n",
    "            dt[\"navigation_data\"][\"longitude\"],\n",
    "            dt[\"navigation_data\"][\"latitude\"],\n",
    "        ),\n",
    "    )\n",
    "    da = da.rename({\"x\":\"longitude\", \"y\":\"latitude\"})\n",
    "    return da\n",
    "\n",
    "def grid_match_SREF(path, dst_crs, dst_shape, dst_transform, variable):\n",
    "    \"\"\"Reproject a Level-2 granule to match a Level-3M-ish granule.\"\"\"\n",
    "    dt = xr.open_datatree(path)\n",
    "    print(dt.keys())\n",
    "    print(dt[\"navigation_data\"])\n",
    "    da = dt[\"geophysical_data\"][variable]\n",
    "    #da = da.assign_coords(wavelength_3d=dt[\"geophysical_data\"][\"wavelength\"].values)\n",
    "    da = da.transpose('wavelength_3d', 'number_of_lines', 'pixels_per_line')\n",
    "    da = da.rio.set_spatial_dims(\"pixels_per_line\", \"number_of_lines\")\n",
    "    da = da.rio.set_crs(\"epsg:4326\")\n",
    "    da = da.rio.reproject(\n",
    "        dst_crs,\n",
    "        shape=dst_shape,\n",
    "        transform=dst_transform,\n",
    "        src_geoloc_array=(\n",
    "            dt[\"navigation_data\"][\"longitude\"],\n",
    "            dt[\"navigation_data\"][\"latitude\"],\n",
    "        ),\n",
    "    )\n",
    "    da = da.rename({\"x\":\"longitude\", \"y\":\"latitude\"})\n",
    "\n",
    "    # Regrid tilt to the new grid\n",
    "    orig_lon = dt[\"navigation_data\"][\"longitude\"].values\n",
    "    orig_lat = dt[\"navigation_data\"][\"latitude\"].values\n",
    "    orig_tilt = dt[\"navigation_data\"][\"tilt\"].values\n",
    "\n",
    "    # Broadcast tilt to match the shape of longitude/latitude if needed\n",
    "    if orig_tilt.ndim == 1 and orig_lon.shape != orig_tilt.shape:\n",
    "        orig_tilt = np.broadcast_to(orig_tilt[:, None], orig_lon.shape)\n",
    "\n",
    "    points = np.column_stack((orig_lon.ravel(), orig_lat.ravel()))\n",
    "    values = orig_tilt.ravel()\n",
    "\n",
    "    # Get the new grid from the reprojected DataArray\n",
    "    new_lon = da[\"longitude\"].values\n",
    "    new_lat = da[\"latitude\"].values\n",
    "\n",
    "    # If new_lon/new_lat are 2D, flatten for griddata, then reshape after\n",
    "    if new_lon.ndim == 2 and new_lat.ndim == 2:\n",
    "        new_points = np.column_stack((new_lon.ravel(), new_lat.ravel()))\n",
    "        tilt_regridded = griddata(points, values, new_points, method='nearest')\n",
    "        tilt_regridded = tilt_regridded.reshape(new_lon.shape)\n",
    "        da = da.assign_coords(tilt=(('latitude', 'longitude'), tilt_regridded))\n",
    "    else:\n",
    "        # If 1D, meshgrid and flatten\n",
    "        lon_grid, lat_grid = np.meshgrid(new_lon, new_lat)\n",
    "        new_points = np.column_stack((lon_grid.ravel(), lat_grid.ravel()))\n",
    "        tilt_regridded = griddata(points, values, new_points, method='nearest')\n",
    "        tilt_regridded = tilt_regridded.reshape(lon_grid.shape)\n",
    "        da = da.assign_coords(tilt=(('latitude', 'longitude'), tilt_regridded))\n",
    "\n",
    "    return da\n",
    "\n",
    "def time_from_attr(ds):\n",
    "    \"\"\"Set the start time attribute as a dataset variable.\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds\n",
    "        a dataset corresponding to a Level-2 granule\n",
    "    \"\"\"\n",
    "    datetime = ds.attrs[\"time_coverage_start\"].replace(\"Z\", \"\")\n",
    "    ds[\"time\"] = ((), np.datetime64(datetime, \"ns\"))\n",
    "    ds = ds.set_coords(\"time\")\n",
    "    return ds\n",
    "\n",
    "def load_first(path, var):\n",
    "    '''Load the first file returned in earthdata search, then manipulate to L3M-like, and\n",
    "       store the crs, shape, and transform to make opening all the search result granules fast'''\n",
    "    datatree = xr.open_datatree(path)\n",
    "    dataset = xr.merge(datatree.to_dict().values())\n",
    "    dataset = dataset.set_coords((\"longitude\", \"latitude\"))\n",
    "\n",
    "    var_data = dataset[var]# use code from dask_gridding notebook to transform L2 granule to L3M-like grid\n",
    "    var_data = var_data.rio.set_spatial_dims(\"pixels_per_line\", \"number_of_lines\")\n",
    "    var_data = var_data.rio.write_crs(\"epsg:4326\")\n",
    "    var_L3M = var_data.rio.reproject(\n",
    "        dst_crs=\"epsg:4326\",\n",
    "        src_geoloc_array=(\n",
    "            var_data.coords[\"longitude\"],\n",
    "            var_data.coords[\"latitude\"],\n",
    "        ),\n",
    "    )\n",
    "    var_L3M = var_L3M.rename({\"x\":\"longitude\", \"y\":\"latitude\"})\n",
    "\n",
    "    var_L3M_aoi = var_L3M.sel({\"longitude\": slice(bbox[0], bbox[2]),\"latitude\": slice(bbox[3], bbox[1])})\n",
    "\n",
    "    crs = var_L3M_aoi.rio.crs# set mapping parameters from newly transformed file, to use when opening the rest with dask\n",
    "    shape = var_L3M_aoi.rio.shape\n",
    "    transform = var_L3M_aoi.rio.transform()\n",
    "\n",
    "    return crs, shape, transform\n",
    "\n",
    "def load_first_SREF(path, var):\n",
    "    '''Load the first file returned in earthdata search, then manipulate to L3M-like, and\n",
    "       store the crs, shape, and transform to make opening all the search result granules fast'''\n",
    "    datatree = xr.open_datatree(path)\n",
    "    dataset = xr.merge(datatree.to_dict().values())\n",
    "    dataset = dataset.set_coords((\"longitude\", \"latitude\"))\n",
    "    print(dataset[var].coords)\n",
    "\n",
    "    var_data = dataset[var]# use code from dask_gridding notebook to transform L2 granule to L3M-like grid\n",
    "    var_data = var_data.transpose('wavelength_3d', 'number_of_lines', 'pixels_per_line')\n",
    "    var_data = var_data.rio.set_spatial_dims(\"pixels_per_line\", \"number_of_lines\")\n",
    "    var_data = var_data.rio.write_crs(\"epsg:4326\")\n",
    "    var_L3M = var_data.rio.reproject(\n",
    "        dst_crs=\"epsg:4326\",\n",
    "        src_geoloc_array=(\n",
    "            var_data.coords[\"longitude\"],\n",
    "            var_data.coords[\"latitude\"],\n",
    "        ),\n",
    "    )\n",
    "    var_L3M = var_L3M.rename({\"x\":\"longitude\", \"y\":\"latitude\"})\n",
    "\n",
    "    var_L3M_aoi = var_L3M.sel({\"longitude\": slice(bbox[0], bbox[2]),\"latitude\": slice(bbox[3], bbox[1])})\n",
    "\n",
    "    crs = var_L3M_aoi.rio.crs# set mapping parameters from newly transformed file, to use when opening the rest with dask\n",
    "    shape = var_L3M_aoi.rio.shape\n",
    "    wl = var_L3M_aoi['wavelength_3d']\n",
    "    transform = var_L3M_aoi.rio.transform()\n",
    "\n",
    "    return crs, shape, transform, wl\n",
    "\n",
    "def coregister_granules(paths, crs, shape, transform, var_name):\n",
    "    ''' Use geographic parameters returned from load_first function to open all the granules for a given \"paths\" \n",
    "        Variable returned by earthdata search. returns an xarray dataset, where the time dimension differentiates\n",
    "        data from each different granule\n",
    "    \n",
    "    '''\n",
    "\n",
    "    client = Client()\n",
    "    futures = client.map(grid_match,paths,dst_crs=crs,dst_shape=shape,dst_transform=transform, variable = var_name)\n",
    "    kwargs = {\"combine\": \"nested\", \"concat_dim\": \"time\"}\n",
    "    attrs = xr.open_mfdataset(paths, preprocess=time_from_attr, **kwargs)\n",
    "    data = xr.combine_nested(client.gather(futures), concat_dim=\"time\")# open all pre files. they are stored in same xarray dataset at different \"time\" coordinates \n",
    "    data[\"time\"] = attrs[\"time\"]\n",
    "    client.close()\n",
    "    print('loaded pre event files')\n",
    "\n",
    "    return data\n",
    "\n",
    "def coregister_granules_SREF(paths, crs, shape, transform, var_name):\n",
    "    ''' Use geographic parameters returned from load_first function to open all the granules for a given \"paths\" \n",
    "        Variable returned by earthdata search. returns an xarray dataset, where the time dimension differentiates\n",
    "        data from each different granule\n",
    "    \n",
    "    '''\n",
    "\n",
    "    client = Client()\n",
    "    print('***************')\n",
    "    futures = client.map(grid_match_SREF,paths,dst_crs=crs,dst_shape=shape,dst_transform=transform, variable = var_name)\n",
    "    kwargs = {\"combine\": \"nested\", \"concat_dim\": \"time\"}\n",
    "    attrs = xr.open_mfdataset(paths, preprocess=time_from_attr, **kwargs)\n",
    "    data = xr.combine_nested(client.gather(futures), concat_dim=\"time\")# open all pre files. they are stored in same xarray dataset at different \"time\" coordinates \n",
    "    data[\"time\"] = attrs[\"time\"]\n",
    "    client.close()\n",
    "    print('loaded pre event files')\n",
    "\n",
    "    return data\n",
    "\n",
    "def enhance(rgb, scale = 0.01, vmin = 0.01, vmax = 1.04, gamma=0.95, contrast=1.2, brightness=1.1, sharpness=2, saturation=1.1):\n",
    "    \"\"\"The SeaDAS recipe for RGB images from Ocean Color missions.\n",
    "\n",
    "    Args:\n",
    "        rgb: a data array with three dimensions, having 3 or 4 bands in the third dimension\n",
    "        scale: scale value for the log transform\n",
    "        vmin: minimum pixel value for the image\n",
    "        vmax: maximum pixel value for the image\n",
    "        gamma: exponential factor for gamma correction\n",
    "        contrast: amount of pixel value differentiation \n",
    "        brightness: pixel values (intensity)\n",
    "        sharpness: amount of detail\n",
    "        saturation: color intensity\n",
    "\n",
    "    Returns:\n",
    "       a transformed data array better for RGB display\n",
    "    \"\"\"\n",
    "    rgb = rgb.where(rgb > 0)\n",
    "    rgb = np.log(rgb / scale) / np.log(1 / scale)\n",
    "    rgb = rgb.where(rgb >= vmin, vmin)\n",
    "    rgb = rgb.where(rgb <= vmax, vmax)    \n",
    "    rgb_min = rgb.min((\"latitude\", \"longitude\"))\n",
    "    rgb_max = rgb.max((\"latitude\", \"longitude\"))\n",
    "    rgb = (rgb - rgb_min) / (rgb_max - rgb_min)\n",
    "    rgb = rgb * gamma\n",
    "    img = rgb * 255\n",
    "    img = img.where(img.notnull(), 0).astype(\"uint8\")\n",
    "    img = Image.fromarray(img.data)\n",
    "    enhancer = ImageEnhance.Contrast(img)\n",
    "    img = enhancer.enhance(contrast)\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    img = enhancer.enhance(brightness)\n",
    "    enhancer = ImageEnhance.Sharpness(img)\n",
    "    img = enhancer.enhance(sharpness)\n",
    "    enhancer = ImageEnhance.Color(img)\n",
    "    img = enhancer.enhance(saturation)\n",
    "    rgb[:] = np.array(img) / 255\n",
    "    return rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b32ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User definitions:\n",
    "\n",
    "pre_tspan = (\"2025-08-18\", \"2025-08-20\")    # define your pre-event and post-event timespans\n",
    "post_tspan = (\"2025-08-21\", \"2025-08-22\")\n",
    "post_post_tspan = (\"2025-08-24\", \"2025-08-28\") # in case you want to look at a longer post-event period\n",
    "\n",
    "min_lon = -82       # Set lat/lon extent for area of interest\n",
    "min_lat = 25\n",
    "max_lon = -63\n",
    "max_lat = 33\n",
    "\n",
    "# OCI suite and variable of interest within a particular suite\n",
    "suite_name=\"PACE_OCI_L2_BGC_NRT\" # shortname of the suite, e.g. For PACE: PACE_OCI_L2_BGC_NRT, PACE_OCI_L2_AOP_NRT, etc. (plan to add more to this comment)\n",
    "var_name = \"chlor_a\" # variable of interest\n",
    "\n",
    "in_the_cloud = False # set to true if running in cloud (e.g. Cryocloud), false if running locally. For speed, granules will be downloaded to a local_data directory when set to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef8781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the earthdata search:\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "pre_results = earthaccess.search_data(\n",
    "    short_name=suite_name,\n",
    "    temporal=pre_tspan,\n",
    "    bounding_box=bbox,\n",
    ")\n",
    "print(\" Number of pre-event granules: \"+str(len(pre_results)))\n",
    "\n",
    "post_results = earthaccess.search_data(\n",
    "    short_name=suite_name,\n",
    "    temporal=post_tspan,\n",
    "    bounding_box=bbox,\n",
    ")\n",
    "print(\" Number of post-event granules: \"+str(len(post_results)))\n",
    "\n",
    "post_post_results = earthaccess.search_data(\n",
    "    short_name=suite_name,\n",
    "    temporal=post_post_tspan,\n",
    "    bounding_box=bbox,\n",
    ")\n",
    "print(\" Number of post-event granules: \"+str(len(post_results)))\n",
    "\n",
    "# if not in the cloud, make a folder to downlad granules to. This folder is configured in the .gitignore to not be tracked\n",
    "if in_the_cloud == False:\n",
    "    os.makedirs('local_data/', exist_ok=True)\n",
    "\n",
    "# in running this script in the cloud, earthaccess.open is fast, but when running locally it takes a long time (10+ min), and locally downloading the data is faster.\n",
    "# so, get the data with different approaches, depening on whether you are in the cloud or not, defined above\n",
    "if in_the_cloud == True:\n",
    "    pre_paths = earthaccess.open(pre_results)\n",
    "    post_paths = earthaccess.open(post_results)\n",
    "    post_post_path = earthaccess.open(post_post_results)\n",
    "else:\n",
    "    pre_paths = earthaccess.download(pre_results, local_path=\"local_data/\")\n",
    "    post_paths = earthaccess.download(post_results, local_path=\"local_data/\")\n",
    "    post_post_paths = earthaccess.download(post_post_results, local_path=\"local_data/\")\n",
    "    print('Files Downloaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55279a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the geospatial characteristics from the first file in paths using 1-line function\n",
    "# This cell is steps is parts 1, 2a, and 2b from previous workflow. functions clean things up nicely\n",
    "\n",
    "crs, shape, transform = load_first(pre_paths[0], var_name)# once set, dont need to rerun for other vars in same suite\n",
    "\n",
    "pre_data = coregister_granules(pre_paths, crs, shape, transform, var_name)\n",
    "post_data = coregister_granules(post_paths, crs, shape, transform, var_name)\n",
    "post_post_data = coregister_granules(post_post_paths, crs, shape, transform, var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 2, figsize=(15, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "fig,ax = plt.subplots(1, 2, figsize=(15, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax[0].gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = pre_data.mean(\"time\").plot(x=\"longitude\", y=\"latitude\" , cbar_kwargs={'label': 'Chlor mg/m3', 'shrink':0.5}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax[0], robust=True)\n",
    "ax[0].set_xlim(bbox[0]-3,bbox[2]+3)\n",
    "ax[0].set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax[0].coastlines()\n",
    "ax[0].set_title('Pre_hurricane Mean (8/15 - 8/20)')\n",
    "\n",
    "ax[1].gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = post_data.mean(\"time\").plot(x=\"longitude\", y=\"latitude\", cbar_kwargs={'label': 'Chlor mg/m3', 'shrink':0.5}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax[1], robust=True)\n",
    "ax[1].set_xlim(bbox[0]-3,bbox[2]+3)\n",
    "ax[1].set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax[1].coastlines()\n",
    "ax[1].set_title('Post Hurricane Mean (8/21 - 8/22)')\n",
    "\n",
    "fig,ax = plt.subplots(1, 1, figsize=(7.5, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = post_post_data.mean(\"time\").plot(x=\"longitude\", y=\"latitude\", cbar_kwargs={'label': 'Chlor mg/m3', 'shrink':0.5}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax, robust=True)\n",
    "ax.set_xlim(bbox[0]-3,bbox[2]+3,)\n",
    "ax.set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax.coastlines()\n",
    "ax.set_title('~ 1 Week After Hurricane Mean (8/21 - 8/28)')\n",
    "\n",
    "for i, data_slice in enumerate(pre_data):\n",
    "    fig, ax = plt.subplots(figsize=(5, 4), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "    plot = data_slice.plot(\n",
    "        x=\"longitude\", y=\"latitude\",\n",
    "        cbar_kwargs={'label': 'Chlor mg/m3', 'shrink': 0.5},\n",
    "        cmap=cmocean.cm.haline,\n",
    "        norm=LogNorm(vmin=.01, vmax=5),\n",
    "        ax=ax,\n",
    "        robust=True\n",
    "    )\n",
    "    ax.set_xlim(bbox[0]-3, bbox[2]+3)\n",
    "    ax.set_ylim(bbox[1]-3, bbox[3]+3)\n",
    "    ax.coastlines()\n",
    "    ax.set_title(f'Granule {i+1} - Time: {str(data_slice.time.values)[:19]}')\n",
    "    plt.show()\n",
    "\n",
    "for i, data_slice in enumerate(post_data):\n",
    "    fig, ax = plt.subplots(figsize=(5, 4), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "    plot = data_slice.plot(\n",
    "        x=\"longitude\", y=\"latitude\",\n",
    "        cbar_kwargs={'label': 'Chlor mg/m3', 'shrink': 0.5},\n",
    "        cmap=cmocean.cm.haline,\n",
    "        norm=LogNorm(vmin=.01, vmax=5),\n",
    "        ax=ax,\n",
    "        robust=True\n",
    "    )\n",
    "    ax.set_xlim(bbox[0]-3, bbox[2]+3)\n",
    "    ax.set_ylim(bbox[1]-3, bbox[3]+3)\n",
    "    ax.coastlines()\n",
    "    ax.set_title(f'Granule {i+1} - Time: {str(data_slice.time.values)[:19]}')\n",
    "    plt.show()\n",
    "\n",
    "for i, data_slice in enumerate(post_post_data):\n",
    "    fig, ax = plt.subplots(figsize=(5, 4), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "    plot = data_slice.plot(\n",
    "        x=\"longitude\", y=\"latitude\",\n",
    "        cbar_kwargs={'label': 'Chlor mg/m3', 'shrink': 0.5},\n",
    "        cmap=cmocean.cm.haline,\n",
    "        norm=LogNorm(vmin=.01, vmax=5),\n",
    "        ax=ax,\n",
    "        robust=True\n",
    "    )\n",
    "    ax.set_xlim(bbox[0]-3, bbox[2]+3)\n",
    "    ax.set_ylim(bbox[1]-3, bbox[3]+3)\n",
    "    ax.coastlines()\n",
    "    ax.set_title(f'Granule {i+1} - Time: {str(data_slice.time.values)[:19]}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96378d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the pre and post event data. first, mask both to only pixels where data is available before and after the hurricane\n",
    "# then, mask each where it's values are greater or less than\n",
    "pre_mean = pre_data.mean(\"time\")\n",
    "post_mean = post_data.mean(\"time\")\n",
    "post_post_mean = post_post_data.mean(\"time\")\n",
    "\n",
    "# Change mean values equal to zero to NaN, so they don't affect percent change calculations\n",
    "# pre_mean = pre_mean.where(pre_mean != 0)\n",
    "# post_mean = post_mean.where(post_mean != 0)\n",
    "# post_post_mean = post_post_mean.where(post_post_mean != 0)\n",
    "\n",
    "# post_mean['longitude'] = post_mean['longitude'].round(5)# round lat/lon to 5 decimals, was having issues with floats at large # of decimals not being exactly the same\n",
    "# post_mean['latitude'] = post_mean['latitude'].round(5)\n",
    "# pre_mean['longitude'] = pre_mean['longitude'].round(5)\n",
    "# pre_mean['latitude'] = pre_mean['latitude'].round(5)\n",
    "# post_post_mean['longitude'] = post_post_mean['longitude'].round(5)\n",
    "# post_post_mean['latitude'] = post_post_mean['latitude'].round(5)\n",
    "\n",
    "# Create a mask where both datasets have valid values\n",
    "mask = ~np.isnan(post_mean) & ~np.isnan(pre_mean)\n",
    "mask2 = ~np.isnan(post_post_mean) & ~np.isnan(pre_mean)\n",
    "\n",
    "# Apply mask to both datasets\n",
    "pre_mean_mask = pre_mean.where(mask)\n",
    "post_mean_mask = post_mean.where(mask)\n",
    "post_post_mean_mask = post_post_mean.where(mask2)\n",
    "\n",
    "post_greater = (post_mean_mask-pre_mean_mask)/pre_mean_mask * 100\n",
    "post_post_greater = (post_post_mean_mask-pre_mean_mask)/pre_mean_mask * 100\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(7.5, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = post_greater.plot(x=\"longitude\", y=\"latitude\" , cbar_kwargs={'label': 'Percent Change', 'shrink':0.5}, cmap=cmocean.cm.balance, extend = \"neither\", ax=ax, robust=True)\n",
    "ax.set_xlim(bbox[0]-3,bbox[2]+3)\n",
    "ax.set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax.coastlines()\n",
    "ax.set_title('Percent Change in Chlorphyll-a')\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(7.5, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = post_post_greater.plot(x=\"longitude\", y=\"latitude\" , cbar_kwargs={'label': 'Percent Change', 'shrink':0.5}, cmap=cmocean.cm.balance, extend = \"neither\", ax=ax, robust=True)\n",
    "ax.set_xlim(bbox[0]-3,bbox[2]+3)\n",
    "ax.set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax.coastlines()\n",
    "ax.set_title('Percent Change in Chlorphyll-a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25214c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same analysis, but for AVW\n",
    "\n",
    "pre_tspan = (\"2025-08-18\", \"2025-08-20\")    # define your pre-event and post-event timespans\n",
    "post_tspan = (\"2025-08-21\", \"2025-08-22\")\n",
    "\n",
    "min_lon = -82       # Set lat/lon extent for area of interest\n",
    "min_lat = 27.8\n",
    "max_lon = -70\n",
    "max_lat = 32.1\n",
    "\n",
    "suite_name=\"PACE_OCI_L2_AOP_NRT\" # oci suite name\n",
    "var_name = \"avw\" # variable of interest\n",
    "\n",
    "in_the_cloud = False # set to true if crunning in cloud (e.g. Cryocloud), false if running locally. For speed, granules will be downloaded to a local_data directory when set to false\n",
    "\n",
    "# do the earthdata search\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)\n",
    "if in_the_cloud == False:\n",
    "    os.makedirs('local_data/', exist_ok=True)# if not in the cloud, make a folder to downlad granules to. This folder is configured in the .gitignore to not be tracked\n",
    "\n",
    "pre_results = earthaccess.search_data(\n",
    "    short_name=suite_name,\n",
    "    temporal=pre_tspan,\n",
    "    bounding_box=bbox,\n",
    ")\n",
    "print(\" Number of pre-event granules: \"+str(len(pre_results)))\n",
    "\n",
    "post_results = earthaccess.search_data(\n",
    "    short_name=suite_name,\n",
    "    temporal=post_tspan,\n",
    "    bounding_box=bbox,\n",
    ")\n",
    "print(\" Number of post-event granules: \"+str(len(post_results)))\n",
    "\n",
    "# in running this script in the cloud, earthaccess.open is fast, but when running locally it takes a long time (10+ min), and locally downloading the data is faster.\n",
    "# so, get the data with different approaches, depening on whether you are in the cloud or not, defined above\n",
    "if in_the_cloud == True:\n",
    "    pre_paths = earthaccess.open(pre_results)\n",
    "    post_paths = earthaccess.open(post_results)\n",
    "else:\n",
    "    pre_paths = earthaccess.download(pre_results, local_path=\"local_data/\")\n",
    "    post_paths = earthaccess.download(post_results, local_path=\"local_data/\")\n",
    "    print('Files Downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca23e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the geospatial characteristics from the first file in paths using 1-line function\n",
    "# This cell is steps is parts 1, 2a, and 2b from previous workflow. functions clean things up nicely\n",
    "\n",
    "crs, shape, transform = load_first(pre_paths[0], var_name)# once set, dont need to rerun for other vars in same suite\n",
    "\n",
    "pre_data = coregister_granules(pre_paths, crs, shape, transform, var_name)\n",
    "post_data = coregister_granules(post_paths, crs, shape, transform, var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax[0].gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = pre_data.mean(\"time\").plot(x=\"longitude\", y=\"latitude\", extend=\"neither\", cbar_kwargs={'label': 'AVW [nm]', 'shrink':0.5}, cmap=\"turbo\", vmin=400, vmax=700, ax=ax[0], robust=True)\n",
    "ax[0].set_xlim(bbox[0]-3,bbox[2]+3)\n",
    "ax[0].set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax[0].coastlines()\n",
    "ax[0].set_title('pre_hurricane_8/18 & 8/19')\n",
    "\n",
    "\n",
    "ax[1].gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = post_data.mean(\"time\").plot(x=\"longitude\", y=\"latitude\", extend=\"neither\", cbar_kwargs={'label': 'AVW [nm]', 'shrink':0.5}, cmap=\"turbo\", vmin=400, vmax=700, ax=ax[1], robust=True)\n",
    "ax[1].set_xlim(bbox[0]-3,bbox[2]+3)\n",
    "ax[1].set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax[1].coastlines()\n",
    "ax[1].set_title('post_hurricane 8/21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af67691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the pre and post event data. first, mask both to only pixels where data is available before and after the hurricane\n",
    "# then, mask each where it's values are greater or less than\n",
    "\n",
    "pre_mean = pre_data.mean(\"time\")\n",
    "post_mean = post_data.mean(\"time\")\n",
    "\n",
    "post_mean['longitude'] = post_mean['longitude'].round(5)# round lat/lon to 5 decimals, was having issues with floats at large # of decimals not being exactly the same\n",
    "post_mean['latitude'] = post_mean['latitude'].round(5)\n",
    "pre_mean['longitude'] = pre_mean['longitude'].round(5)\n",
    "pre_mean['latitude'] = pre_mean['latitude'].round(5)\n",
    "\n",
    "# Create a mask where both datasets have valid values\n",
    "mask = ~np.isnan(post_mean) & ~np.isnan(pre_mean)\n",
    "\n",
    "# Apply mask to both datasets\n",
    "post_mean_mask = post_mean.where(mask)\n",
    "pre_mean_mask = pre_mean.where(mask)\n",
    "\n",
    "post_greater = post_mean_mask-pre_mean_mask\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(15, 12), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = post_greater.plot(x=\"longitude\", y=\"latitude\" , cbar_kwargs={'label': 'AVW Change [nm]', 'shrink':0.5}, cmap=cmocean.cm.delta, extend = \"neither\", ax=ax, robust=True)\n",
    "ax.set_xlim(bbox[0]-3,bbox[2]+3,)\n",
    "ax.set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax.coastlines()\n",
    "ax.set_title('Change in AVW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f2747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5e020be",
   "metadata": {},
   "source": [
    "## Stitch True Color Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c020b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User definitions:\n",
    "pre_tspan = (\"2025-08-19\", \"2025-08-19\")    # define your pre-event and post-event timespans\n",
    "\n",
    "min_lon = -82       # Set lat/lon extent for area of interest\n",
    "min_lat = 25\n",
    "max_lon = -63\n",
    "max_lat = 33\n",
    "\n",
    "# OCI suite and variable of interest within a particular suite\n",
    "suite_name=\"PACE_OCI_L2_SFREFL_NRT\" # shortname of the suite, e.g. For PACE: PACE_OCI_L2_SFREFL_NRT, PACE_OCI_L2_BGC_NRT, PACE_OCI_L2_AOP_NRT, etc. (plan to add more to this comment)\n",
    "var_name = \"rhos\" # variable of interest\n",
    "\n",
    "in_the_cloud = False # set to true if running in cloud (e.g. Cryocloud), false if running locally. For speed, granules will be downloaded to a local_data directory when set to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bfe291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the earthdata search:\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "results = earthaccess.search_data(\n",
    "    short_name=suite_name,\n",
    "    temporal=pre_tspan,\n",
    "    bounding_box=bbox,\n",
    ")\n",
    "print(\" Number of pre-event granules: \"+str(len(results)))\n",
    "\n",
    "# if not in the cloud, make a folder to downlad granules to. This folder is configured in the .gitignore to not be tracked\n",
    "if in_the_cloud == False:\n",
    "    os.makedirs('local_data/', exist_ok=True)\n",
    "\n",
    "# in running this script in the cloud, earthaccess.open is fast, but when running locally it takes a long time (10+ min), and locally downloading the data is faster.\n",
    "# so, get the data with different approaches, depening on whether you are in the cloud or not, defined above\n",
    "if in_the_cloud == True:\n",
    "    pre_paths = earthaccess.open(results)\n",
    "else:\n",
    "    pre_paths = earthaccess.download(results, local_path=\"local_data/\")\n",
    "\n",
    "print('Files Downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debd65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the geospatial characteristics from the first file in paths using 1-line function\n",
    "# This cell is steps is parts 1, 2a, and 2b from previous workflow. functions clean things up nicely\n",
    "\n",
    "crs, shape, transform, wl = load_first_SREF(pre_paths[0], var_name)# once set, dont need to rerun for other vars in same suite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11626de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = coregister_granules_SREF(pre_paths, crs, shape, transform, var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ce6007",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_2 = pre_data.assign_coords(wavelength_3d=wl)\n",
    "rhos_rgb = pre_data_2.sel(wavelength_3d=[645, 555, 445], method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = rhos_rgb.transpose(\"time\", \"latitude\", \"longitude\", \"wavelength_3d\")\n",
    "\n",
    "for i, data_slice in enumerate(img_array):\n",
    "    rgb = enhance(data_slice)\n",
    "\n",
    "    # Mask pixels where all channels are zero (fully black regions)\n",
    "    zero_mask = (rgb == 0).all(dim=\"wavelength_3d\")\n",
    "    rgb = rgb.where(~zero_mask)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    axes = plt.subplot(projection=ccrs.PlateCarree())\n",
    "    artist = axes.pcolormesh(\n",
    "        rgb[\"longitude\"],\n",
    "        rgb[\"latitude\"],\n",
    "        rgb,\n",
    "        shading=\"nearest\",\n",
    "        rasterized=True,\n",
    "        )\n",
    "    axes.set_aspect(\"equal\")\n",
    "    axes.set_xlim(bbox[0]-3,bbox[2]+3)\n",
    "    axes.set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "    axes.coastlines()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02561b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_array = rhos_rgb.transpose(\"time\", \"latitude\", \"longitude\", \"wavelength_3d\")\n",
    "# zero_mask = (img_array == 0).all(dim=\"wavelength_3d\")\n",
    "# img_array = img_array.where(~zero_mask)\n",
    "# img_array = img_array.mean(\"time\")\n",
    "\n",
    "img_array = rhos_rgb.transpose(\"time\", \"latitude\", \"longitude\", \"wavelength_3d\")\n",
    "zero_mask = (img_array == 0).all(dim=\"wavelength_3d\")\n",
    "img_array = img_array.where(~zero_mask)\n",
    "img_array = img_array.ffill(\"time\").isel(time=-1)\n",
    "\n",
    "rgb = enhance(img_array)\n",
    "\n",
    "# Mask pixels where all channels are zero (fully black regions)\n",
    "zero_mask = (rgb == 0).all(dim=\"wavelength_3d\")\n",
    "rgb = rgb.where(~zero_mask)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "axes = plt.subplot(projection=ccrs.PlateCarree())\n",
    "artist = axes.pcolormesh(\n",
    "    rgb[\"longitude\"],\n",
    "    rgb[\"latitude\"],\n",
    "    rgb,\n",
    "    shading=\"nearest\",\n",
    "    rasterized=True,\n",
    "    )\n",
    "axes.set_aspect(\"equal\")\n",
    "axes.set_xlim(bbox[0]-3,bbox[2]+3)\n",
    "axes.set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "axes.coastlines()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19956173",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = img_array[\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985bf484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackweek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
