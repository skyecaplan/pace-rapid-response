{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c5f681",
   "metadata": {},
   "source": [
    "### PACE Rapid Response Notebook - for workshopping workflow\n",
    "\n",
    "To Do:\n",
    "- adapt hackweek code to pull pace data for a given product, lat/lon extent, and time range defined before and after an event of interest\n",
    "- spatially bin l2 data consistently?\n",
    "- Make a mask of only pixels present both datasets (to prevent bias), compare data w/ pretty maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e94b480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import earthaccess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib.colors import LogNorm\n",
    "import cmocean\n",
    "from dask.distributed import Client\n",
    "from matplotlib.patches import Rectangle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796357c9",
   "metadata": {},
   "source": [
    "### Development test-case\n",
    "Hurricane Erin, look at chl-a before and after hurricane for MAB\n",
    "Timeline (using worldview): \n",
    "- Hurricane off florida/gulf stream PACE imagery on AUG 20\n",
    "- AUG 21, hurricane passed and clear imagery\n",
    "- Aug 18/19 hurricane not there yet\n",
    "\n",
    "Lat/Lon bounds: \n",
    "UL:32.09, -76.63 \n",
    "LR:27.78, -77.80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36b2d7",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "- use a datapoint in the center of AOI and time bounds/ earthaccess search to determine a granule form which we will define our L3M-like grid (see dask_gridding tutorial notebook)\n",
    "- define crs, tramsform, shape from this L3M-like granule (chla_L3M_aoi in dask_griddding.ipynb notebook from hackweek)\n",
    "- do another earthdata search, for full AOI over time period (1 week ish pre hurricane)\n",
    "- open all and calculate average of parameter (in this case chl-a) using dask (follow dask_griddding.ipynb section 4)\n",
    "- repeat above with timespan post hurricane, using same crs, tramsform, shape for gridding\n",
    "- at this point, have pre and post hurricane chl averages on same spatial grid. now, mask each for pixels that have date pre and post hurricane\n",
    "- make graphic showing before/after. Calculate average pre and post (using mask to avoid bias) and calculate a % increase in chl-a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112534cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newer workflow - test making the geographic parameter and dask steps functions\n",
    "\n",
    "# all functions initilized here:\n",
    "\n",
    "# Define Functions, don't modify\n",
    "def grid_match(path, dst_crs, dst_shape, dst_transform, variable):\n",
    "    \"\"\"Reproject a Level-2 granule to match a Level-3M-ish granule.\"\"\"\n",
    "    dt = xr.open_datatree(path)\n",
    "    da = dt[\"geophysical_data\"][variable]\n",
    "    da = da.rio.set_spatial_dims(\"pixels_per_line\", \"number_of_lines\")\n",
    "    da = da.rio.set_crs(\"epsg:4326\")\n",
    "    da = da.rio.reproject(\n",
    "        dst_crs,\n",
    "        shape=dst_shape,\n",
    "        transform=dst_transform,\n",
    "        src_geoloc_array=(\n",
    "            dt[\"navigation_data\"][\"longitude\"],\n",
    "            dt[\"navigation_data\"][\"latitude\"],\n",
    "        ),\n",
    "    )\n",
    "    da = da.rename({\"x\":\"longitude\", \"y\":\"latitude\"})\n",
    "    return da\n",
    "\n",
    "def time_from_attr(ds):\n",
    "    \"\"\"Set the start time attribute as a dataset variable.\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds\n",
    "        a dataset corresponding to a Level-2 granule\n",
    "    \"\"\"\n",
    "    datetime = ds.attrs[\"time_coverage_start\"].replace(\"Z\", \"\")\n",
    "    ds[\"time\"] = ((), np.datetime64(datetime, \"ns\"))\n",
    "    ds = ds.set_coords(\"time\")\n",
    "    return ds\n",
    "\n",
    "def load_first(path, var):\n",
    "    '''Load the first file returned in earthdata search, then manipulate to L3M-like, and\n",
    "       store the crs, shape, and transform to make opening all the search result granules fast'''\n",
    "    datatree = xr.open_datatree(path)\n",
    "    dataset = xr.merge(datatree.to_dict().values())\n",
    "    dataset = dataset.set_coords((\"longitude\", \"latitude\"))\n",
    "\n",
    "    var_data = dataset[var]# use code from dask_gridding notebook to transform L2 granule to L3M-like grid\n",
    "    var_data = var_data.rio.set_spatial_dims(\"pixels_per_line\", \"number_of_lines\")\n",
    "    var_data = var_data.rio.write_crs(\"epsg:4326\")\n",
    "    var_L3M = var_data.rio.reproject(\n",
    "        dst_crs=\"epsg:4326\",\n",
    "        src_geoloc_array=(\n",
    "            var_data.coords[\"longitude\"],\n",
    "            var_data.coords[\"latitude\"],\n",
    "        ),\n",
    "    )\n",
    "    var_L3M = var_L3M.rename({\"x\":\"longitude\", \"y\":\"latitude\"})\n",
    "\n",
    "    var_L3M_aoi = var_L3M.sel({\"longitude\": slice(bbox[0], bbox[2]),\"latitude\": slice(bbox[3], bbox[1])})\n",
    "\n",
    "    crs = var_L3M_aoi.rio.crs# set mapping parameters from newly transformed file, to use when opening the rest with dask\n",
    "    shape = var_L3M_aoi.rio.shape\n",
    "    transform = var_L3M_aoi.rio.transform()\n",
    "\n",
    "    return crs, shape, transform\n",
    "\n",
    "def coregister_granules(paths, crs, shape, transform, var_name):\n",
    "    ''' Use geographic parameters returned from load_first function to open all the granules for a given \"paths\" \n",
    "        Variable returned by earthdata search. returns an xarray dataset, where the time dimension differentiates\n",
    "        data from each different granule\n",
    "    \n",
    "    '''\n",
    "\n",
    "    client = Client()\n",
    "    futures = client.map(grid_match,paths,dst_crs=crs,dst_shape=shape,dst_transform=transform, variable = var_name)\n",
    "    kwargs = {\"combine\": \"nested\", \"concat_dim\": \"time\"}\n",
    "    attrs = xr.open_mfdataset(paths, preprocess=time_from_attr, **kwargs)\n",
    "    data = xr.combine_nested(client.gather(futures), concat_dim=\"time\")# open  all pre files. they are stored in same xarray dataset at different \"time\" coordinates \n",
    "    data[\"time\"] = attrs[\"time\"]\n",
    "    client.close()\n",
    "    print('loaded pre event files')\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b32ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User definitions:\n",
    "\n",
    "pre_tspan = (\"2025-08-18\", \"2025-08-20\")    # define your pre-event and post-event timespans\n",
    "post_tspan = (\"2025-08-21\", \"2025-08-22\")\n",
    "\n",
    "min_lon = -82       # Set lat/lon extent for area of interest\n",
    "min_lat = 27.8\n",
    "max_lon = -70\n",
    "max_lat = 32.1\n",
    "\n",
    "suite_name=\"PACE_OCI_L2_BGC_NRT\" # oci suite name\n",
    "var_name = \"chlor_a\" # variable of interest\n",
    "\n",
    "in_the_cloud = False # set to true if running in cloud (e.g. Cryocloud), false if running locally. For speed, granules will be downloaded to a local_data directory when set to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ef8781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the earthdata search\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)\n",
    "if in_the_cloud == False:\n",
    "    os.makedirs('local_data/', exist_ok=True)# if not in the cloud, make a folder to downlad granules to. This folder is configured in the .gitignore to not be tracked\n",
    "\n",
    "pre_results = earthaccess.search_data(\n",
    "    short_name=suite_name,\n",
    "    temporal=pre_tspan,\n",
    "    bounding_box=bbox,\n",
    ")\n",
    "print(\" Number of pre-event granules: \"+str(len(pre_results)))\n",
    "\n",
    "post_results = earthaccess.search_data(\n",
    "    short_name=suite_name,\n",
    "    temporal=post_tspan,\n",
    "    bounding_box=bbox,\n",
    ")\n",
    "print(\" Number of post-event granules: \"+str(len(post_results)))\n",
    "\n",
    "# in running this script in the cloud, earthaccess.open is fast, but when running locally it takes a long time (10+ min), and locally downloading the data is faster.\n",
    "# so, get the data with different approaches, depening on whether you are in the cloud or not, defined above\n",
    "if in_the_cloud == True:\n",
    "    pre_paths = earthaccess.open(pre_results)\n",
    "    post_paths = earthaccess.open(post_results)\n",
    "else:\n",
    "    pre_paths = earthaccess.download(pre_results, local_path=\"local_data/\")\n",
    "    post_paths = earthaccess.download(post_results, local_path=\"local_data/\")\n",
    "    print('Files Downloaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e55279a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the geospatial characteristics from the first file in paths using 1-line function\n",
    "# This cell is steps is parts 1, 2a, and 2b from previous workflow. functions clean things up nicely\n",
    "\n",
    "crs, shape, transform = load_first(pre_paths[0], var_name)# once set, dont need to rerun for other vars in same suite\n",
    "\n",
    "pre_data = coregister_granules(pre_paths, crs, shape, transform, var_name)\n",
    "post_data = coregister_granules(post_paths, crs, shape, transform, var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4714eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax[0].gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = pre_data.mean(\"time\").plot(x=\"longitude\", y=\"latitude\" , cbar_kwargs={'label': 'Chlor mg/m3', 'shrink':0.5}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax[0], robust=True)\n",
    "ax[0].set_xlim(bbox[0]-3,bbox[2]+3,)\n",
    "ax[0].set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax[0].coastlines()\n",
    "ax[0].set_title('pre_hurricane_8/18 & 8/19')\n",
    "\n",
    "\n",
    "ax[1].gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = post_data.mean(\"time\").plot(x=\"longitude\", y=\"latitude\", cbar_kwargs={'label': 'Chlor mg/m3', 'shrink':0.5}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax[1], robust=True)\n",
    "ax[1].set_xlim(bbox[0]-3,bbox[2]+3,)\n",
    "ax[1].set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax[1].coastlines()\n",
    "ax[1].set_title('post_hurricane 8/21')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e21173",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lon = -70\n",
    "min_lat = 27.8\n",
    "max_lon = -80\n",
    "max_lat = 32.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ae4da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user bounds\n",
    "tspan = (\"2025-08-21\", \"2025-08-24\")# post hurricane time period\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "# math\n",
    "avg_lon = (min_lon + max_lon)/2; avg_lat = (min_lat + max_lat)/2\n",
    "bbox_pt = (avg_lon, avg_lat, avg_lon, avg_lat)# single point at center of defined bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc36f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    short_name=\"PACE_OCI_L2_BGC_NRT\",\n",
    "    temporal=tspan,\n",
    "    bounding_box=bbox_pt,# logic to select central point of bounding box\n",
    ")\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "761922d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = earthaccess.download(results, local_path=\"local_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba702470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths = earthaccess.open(results)\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57d40219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes up to 2 min to run, opens dataset w/ datatree, put all variables together, and fully load dataset to memory to make map plotting faster\n",
    "datatree = xr.open_datatree(paths[0])\n",
    "dataset = xr.merge(datatree.to_dict().values())\n",
    "dataset = dataset.set_coords((\"longitude\", \"latitude\"))\n",
    "dataset = dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14c31d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,7))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = dataset['chlor_a'].plot(x=\"longitude\", y=\"latitude\", cbar_kwargs={'label': 'Chlor mg/m3'}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax)\n",
    "ax.scatter([avg_lon],[avg_lat], c='r' )\n",
    "                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cb19796",
   "metadata": {},
   "outputs": [],
   "source": [
    "chla = dataset[\"chlor_a\"]# use code from dask_gridding notebook to transform L2 granule to L3M-like grid\n",
    "chla = chla.rio.set_spatial_dims(\"pixels_per_line\", \"number_of_lines\")\n",
    "chla = chla.rio.write_crs(\"epsg:4326\")\n",
    "chla_L3M = chla.rio.reproject(\n",
    "    dst_crs=\"epsg:4326\",\n",
    "    src_geoloc_array=(\n",
    "        chla.coords[\"longitude\"],\n",
    "        chla.coords[\"latitude\"],\n",
    "    ),\n",
    ")\n",
    "chla_L3M = chla_L3M.rename({\"x\":\"longitude\", \"y\":\"latitude\"})\n",
    "chla_L3M.plot()\n",
    "print(bbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9364fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chla_L3M_aoi = chla_L3M.sel(\n",
    "    {\n",
    "        \"longitude\": slice(bbox[2], bbox[0]),\n",
    "        \"latitude\": slice(bbox[3], bbox[1]),\n",
    "    },\n",
    ")\n",
    "# slice post hurricane to lat/lon bounds of interest\n",
    "\n",
    "fig = plt.figure(figsize=(16,7))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = chla_L3M_aoi.plot(x=\"longitude\", y=\"latitude\", cbar_kwargs={'label': 'Chlor mg/m3'}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax)\n",
    "ax.scatter([avg_lon],[avg_lat], c='r' )\n",
    "ax.set_xlim(bbox[2]-7,bbox[0]+7,)\n",
    "ax.set_ylim(bbox[1]-7,bbox[3]+7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dee52ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, attempt to open multiple files using the dask approach from hackweek tutorial\n",
    "# the one we already opened is the \"after hurricane\" data\n",
    "# here, try to get 2 clear granules from 18th and 19th, open and L3M grid according to previous one\n",
    "\n",
    "# user bounds\n",
    "tspan = (\"2025-08-18\", \"2025-08-20\")\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "# math\n",
    "avg_lon = (min_lon + max_lon)/2; avg_lat = (min_lat + max_lat)/2\n",
    "bbox_pt = (avg_lon, avg_lat, avg_lon, avg_lat)# single point at center of defined bounds\n",
    "\n",
    "results = earthaccess.search_data(\n",
    "    short_name=\"PACE_OCI_L2_BGC_NRT\",\n",
    "    temporal=tspan,\n",
    "    bounding_box=bbox_pt,# logic to select central point of bounding box\n",
    ")\n",
    "print(bbox_pt)\n",
    "len(results)\n",
    "paths = earthaccess.open(results)\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61126d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions\n",
    "def grid_match(path, dst_crs, dst_shape, dst_transform):\n",
    "    \"\"\"Reproject a Level-2 granule to match a Level-3M-ish granule.\"\"\"\n",
    "    dt = xr.open_datatree(path)\n",
    "    da = dt[\"geophysical_data\"][\"chlor_a\"]\n",
    "    da = da.rio.set_spatial_dims(\"pixels_per_line\", \"number_of_lines\")\n",
    "    da = da.rio.set_crs(\"epsg:4326\")\n",
    "    da = da.rio.reproject(\n",
    "        dst_crs,\n",
    "        shape=dst_shape,\n",
    "        transform=dst_transform,\n",
    "        src_geoloc_array=(\n",
    "            dt[\"navigation_data\"][\"longitude\"],\n",
    "            dt[\"navigation_data\"][\"latitude\"],\n",
    "        ),\n",
    "    )\n",
    "    da = da.rename({\"x\":\"longitude\", \"y\":\"latitude\"})\n",
    "    return da\n",
    "\n",
    "def time_from_attr(ds):\n",
    "    \"\"\"Set the start time attribute as a dataset variable.\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds\n",
    "        a dataset corresponding to a Level-2 granule\n",
    "    \"\"\"\n",
    "    datetime = ds.attrs[\"time_coverage_start\"].replace(\"Z\", \"\")\n",
    "    ds[\"time\"] = ((), np.datetime64(datetime, \"ns\"))\n",
    "    ds = ds.set_coords(\"time\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e58777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13f91cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = chla_L3M_aoi.rio.crs# set mapping parameters from file transformed earlier\n",
    "shape = chla_L3M_aoi.rio.shape\n",
    "transform = chla_L3M_aoi.rio.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11a5f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(\n",
    "    grid_match,\n",
    "    paths,\n",
    "    dst_crs=crs,\n",
    "    dst_shape=shape,\n",
    "    dst_transform=transform,\n",
    ")\n",
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19f2f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"combine\": \"nested\", \"concat_dim\": \"time\"}\n",
    "attrs = xr.open_mfdataset(paths, preprocess=time_from_attr, **kwargs)\n",
    "attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728430b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chla = xr.combine_nested(client.gather(futures), concat_dim=\"time\")# open all 4 files. they are stored in same xarray dataset at different \"time\" coordinates \n",
    "chla[\"time\"] = attrs[\"time\"]\n",
    "chla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the average of the files we just opened (4 in case of this example)\n",
    "\n",
    "fig = plt.figure(figsize=(16,7))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = chla.mean(\"time\").plot(x=\"longitude\", y=\"latitude\", cbar_kwargs={'label': 'Chlor mg/m3'}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax, robust=True)\n",
    "ax.scatter([avg_lon],[avg_lat], c='r' )\n",
    "ax.set_xlim(bbox[2]-7,bbox[0]+7,)\n",
    "ax.set_ylim(bbox[1]-7,bbox[3]+7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08656fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## at this point, have 1 granule representing \"post-hurricane\", and a  nice 2 day avg of pre hurricane.\n",
    "# want to compare the 2\n",
    "# before: chla_L3M_aoi\n",
    "# after chla\n",
    "post_mean = chla.mean(\"time\")\n",
    "\n",
    "post_mean['longitude'] = post_mean['longitude'].round(5)# round lat/lon to 5 decimals, was having issues with floats at large # of decimals not being exactly the same\n",
    "post_mean['latitude'] = post_mean['latitude'].round(5)\n",
    "chla_L3M_aoi['longitude'] = chla_L3M_aoi['longitude'].round(5)\n",
    "chla_L3M_aoi['latitude'] = chla_L3M_aoi['latitude'].round(5)\n",
    "\n",
    "pre_mean = chla_L3M_aoi# rename for simplicity\n",
    "\n",
    "\n",
    "# Create a mask where both datasets have valid values\n",
    "mask = ~np.isnan(post_mean) & ~np.isnan(pre_mean)\n",
    "\n",
    "# Apply mask to both datasets\n",
    "post_mean = post_mean.where(mask)\n",
    "pre_mean = pre_mean.where(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b49ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax[0].gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = pre_mean.plot(x=\"longitude\", y=\"latitude\", cbar_kwargs={'label': 'Chlor mg/m3', 'shrink':0.5}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax[0], robust=True)\n",
    "ax[0].scatter([avg_lon],[avg_lat], c='r' )\n",
    "ax[0].set_xlim(bbox[2]-3,bbox[0]+3,)\n",
    "ax[0].set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax[0].coastlines()\n",
    "ax[0].set_title('pre_hurricane_8/18 & 8/19')\n",
    "\n",
    "\n",
    "ax[1].gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = post_mean.plot(x=\"longitude\", y=\"latitude\", cbar_kwargs={'label': 'Chlor mg/m3', 'shrink':0.5}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax[1], robust=True)\n",
    "ax[1].scatter([avg_lon],[avg_lat], c='r' )\n",
    "ax[1].set_xlim(bbox[2]-3,bbox[0]+3,)\n",
    "ax[1].set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax[1].coastlines()\n",
    "ax[1].set_title('post_hurricane 8/21')\n",
    "\n",
    "\n",
    "# ax.coastlines()\n",
    "# ax.gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "# plot = chla.mean(\"time\").plot(x=\"longitude\", y=\"latitude\", cbar_kwargs={'label': 'Chlor mg/m3'}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax, robust=True)\n",
    "# ax.scatter([avg_lon],[avg_lat], c='r' )\n",
    "# ax.set_xlim(bbox[2]-7,bbox[0]+7,)\n",
    "# ax.set_ylim(bbox[1]-7,bbox[3]+7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29997c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new mask to show where post > pre\n",
    "mask2 = post_mean>pre_mean\n",
    "mask3 = post_mean<pre_mean\n",
    "\n",
    "# Apply mask to both datasets\n",
    "post_mean_masked = post_mean.where(mask2)# locations where chla is higher post hurricane\n",
    "pre_mean_masked = pre_mean.where(mask3)# locations where chla is higher pre hurricane\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax[0].gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = pre_mean_masked.plot(x=\"longitude\", y=\"latitude\", cbar_kwargs={'label': 'Chlor mg/m3', 'shrink':0.5}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax[0], robust=True)\n",
    "ax[0].scatter([avg_lon],[avg_lat], c='r' )\n",
    "ax[0].set_xlim(bbox[2]-3,bbox[0]+3,)\n",
    "ax[0].set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax[0].coastlines()\n",
    "ax[0].set_title('pre_hurricane chla higher')\n",
    "\n",
    "\n",
    "ax[1].gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = post_mean_masked.plot(x=\"longitude\", y=\"latitude\", cbar_kwargs={'label': 'Chlor mg/m3', 'shrink':0.5}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax[1], robust=True)\n",
    "ax[1].scatter([avg_lon],[avg_lat], c='r' )\n",
    "ax[1].set_xlim(bbox[2]-3,bbox[0]+3,)\n",
    "ax[1].set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax[1].coastlines()\n",
    "ax[1].set_title('post_hurricane chla higher')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96378d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackweek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
