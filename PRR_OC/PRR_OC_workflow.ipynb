{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06837f7",
   "metadata": {},
   "source": [
    "# PACE Rapid Response Workflow\n",
    "\n",
    "This Jupyter notebook serves as the first of hopefully many examples to rapidly access, download, review, and develop science data products is support of the NASA PACE Mission. In this notebook we investigate the impacts of Hurricane Erin that formed in mid-August of 2025. In particular we attempt to analyze different ecologically relevant biogeochemical parameters before and after the extreme strom off the southeastern shore of the United States.\n",
    "\n",
    "Scientific Premise: \"Remotely sensed ocean color shows increased concentrations of surface chlorophyll within the cool wakes of the hurricanes, apparently in response to the injection of nutrients and/or biogenic pigments into the oligotrophic surface waters. This increase in post-storm surface chlorophyll concentration usually lasted 2â€“3 weeks before it returned to its nominal pre-hurricane levels\" Citation: https://doi.org/10.1029/2003JC001938\n",
    "\n",
    "Graham Trolley & Matthew Kehrli\n",
    "\n",
    "NASA Ocean Ecology Laboratory\n",
    "\n",
    "September 2025\n",
    "\n",
    "### Case Study Information\n",
    "\n",
    "Hurricane Erin, look at chl-a before and after hurricane off FL coast.Hurricanes tend to increase productivity\n",
    "\n",
    "Timeline (using worldview): \n",
    "- Hurricane off florida/gulf stream PACE imagery on AUG 20\n",
    "- Aug 21, hurricane passed and clear imagery\n",
    "- Aug 18/19 hurricane not there yet\n",
    "\n",
    "*post Aug 21, there is a PACE safehold so no data. But, the imagery from Aug21 immediately post hurricane is enough for this example script\n",
    "\n",
    "\n",
    "### Basic Steps\n",
    "- Use worldview to identify an event and region of interest to examine (basic before/after)\n",
    "- define lat lon bounds, and time range for pre- and post-event averages\n",
    "- earthdata search using parameters. Open the first one by itself to establish gridding params\n",
    "- use dask to open all the pre-event granules\n",
    "- use dask to open all the post-event granules\n",
    "- make some initial analysis plots\n",
    "- return pre and post dataframes to user, to allow them to make more custom plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc67bb",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ae344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import earthaccess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib.colors import LogNorm\n",
    "import cmocean\n",
    "from dask.distributed import Client\n",
    "from matplotlib.patches import Rectangle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a5205",
   "metadata": {},
   "source": [
    "### User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2437ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Functions, don't modify\n",
    "def grid_match(path, dst_crs, dst_shape, dst_transform, variable):\n",
    "    \"\"\"Reproject a Level-2 granule to match a Level-3M-ish granule.\"\"\"\n",
    "    dt = xr.open_datatree(path)\n",
    "    da = dt[\"geophysical_data\"][variable]\n",
    "    da = da.rio.set_spatial_dims(\"pixels_per_line\", \"number_of_lines\")\n",
    "    da = da.rio.set_crs(\"epsg:4326\")\n",
    "    da = da.rio.reproject(\n",
    "        dst_crs,\n",
    "        shape=dst_shape,\n",
    "        transform=dst_transform,\n",
    "        src_geoloc_array=(\n",
    "            dt[\"navigation_data\"][\"longitude\"],\n",
    "            dt[\"navigation_data\"][\"latitude\"],\n",
    "        ),\n",
    "    )\n",
    "    da = da.rename({\"x\":\"longitude\", \"y\":\"latitude\"})\n",
    "    return da\n",
    "\n",
    "def time_from_attr(ds):\n",
    "    \"\"\"Set the start time attribute as a dataset variable.\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds\n",
    "        a dataset corresponding to a Level-2 granule\n",
    "    \"\"\"\n",
    "    datetime = ds.attrs[\"time_coverage_start\"].replace(\"Z\", \"\")\n",
    "    ds[\"time\"] = ((), np.datetime64(datetime, \"ns\"))\n",
    "    ds = ds.set_coords(\"time\")\n",
    "    return ds\n",
    "\n",
    "def load_first(path, var):\n",
    "    '''Load the first file returned in earthdata search, then manipulate to L3M-like, and\n",
    "       store the crs, shape, and transform to make opening all the search result granules fast'''\n",
    "    datatree = xr.open_datatree(path)\n",
    "    dataset = xr.merge(datatree.to_dict().values())\n",
    "    dataset = dataset.set_coords((\"longitude\", \"latitude\"))\n",
    "\n",
    "    var_data = dataset[var]# use code from dask_gridding notebook to transform L2 granule to L3M-like grid\n",
    "    var_data = var_data.rio.set_spatial_dims(\"pixels_per_line\", \"number_of_lines\")\n",
    "    var_data = var_data.rio.write_crs(\"epsg:4326\")\n",
    "    var_L3M = var_data.rio.reproject(\n",
    "        dst_crs=\"epsg:4326\",\n",
    "        src_geoloc_array=(\n",
    "            var_data.coords[\"longitude\"],\n",
    "            var_data.coords[\"latitude\"],\n",
    "        ),\n",
    "    )\n",
    "    var_L3M = var_L3M.rename({\"x\":\"longitude\", \"y\":\"latitude\"})\n",
    "\n",
    "    var_L3M_aoi = var_L3M.sel({\"longitude\": slice(bbox[0], bbox[2]),\"latitude\": slice(bbox[3], bbox[1])})\n",
    "\n",
    "    crs = var_L3M_aoi.rio.crs# set mapping parameters from newly transformed file, to use when opening the rest with dask\n",
    "    shape = var_L3M_aoi.rio.shape\n",
    "    transform = var_L3M_aoi.rio.transform()\n",
    "\n",
    "    return crs, shape, transform\n",
    "\n",
    "def coregister_granules(paths, crs, shape, transform, var_name):\n",
    "    ''' Use geographic parameters returned from load_first function to open all the granules for a given \"paths\" \n",
    "        Variable returned by earthdata search. returns an xarray dataset, where the time dimension differentiates\n",
    "        data from each different granule\n",
    "    \n",
    "    '''\n",
    "\n",
    "    client = Client()\n",
    "    futures = client.map(grid_match,paths,dst_crs=crs,dst_shape=shape,dst_transform=transform, variable = var_name)\n",
    "    kwargs = {\"combine\": \"nested\", \"concat_dim\": \"time\"}\n",
    "    attrs = xr.open_mfdataset(paths, preprocess=time_from_attr, **kwargs)\n",
    "    data = xr.combine_nested(client.gather(futures), concat_dim=\"time\")# open  all pre files. they are stored in same xarray dataset at different \"time\" coordinates \n",
    "    data[\"time\"] = attrs[\"time\"]\n",
    "    client.close()\n",
    "    print('loaded pre event files')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8448a8",
   "metadata": {},
   "source": [
    "### User Inputs: Processing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0154562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User definitions:\n",
    "\n",
    "pre_tspan = (\"2025-08-18\", \"2025-08-20\")    # define your pre-event and post-event timespans\n",
    "post_tspan = (\"2025-08-21\", \"2025-08-22\")\n",
    "\n",
    "min_lon = -82       # Set lat/lon extent for area of interest\n",
    "min_lat = 27.8\n",
    "max_lon = -70\n",
    "max_lat = 32.1\n",
    "\n",
    "suite_name=\"PACE_OCI_L2_BGC_NRT\" # oci suite name\n",
    "var_name = \"chlor_a\" # variable of interest\n",
    "\n",
    "in_the_cloud = False # set to true if running in cloud (e.g. Cryocloud), false if running locally. For speed, granules will be downloaded to a local_data directory when set to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cd379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''One large cell to load in all relevant pre-event and post-event granules\n",
    "Steps:\n",
    "- Earth access search to make lists of pre and post event files\n",
    "- Open first entry of list manually, manipulate to L3M-like format and store relevant grid configuration info\n",
    "- Using dask, open pre and post event granules, recast to L3M-like mapped format, trim to spatial extent, combine 2 datasets\n",
    "- Make some default plots to visualize Pre and Post event data\n",
    "'''\n",
    "\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)\n",
    "if in_the_cloud == False:\n",
    "    os.makedirs('local_data/', exist_ok=True)# if not in the cloud, make a folder to downlad granules to. This folder is configured to not be tracked in the gitignore\n",
    "\n",
    "pre_results = earthaccess.search_data(\n",
    "    short_name=suite_name,\n",
    "    temporal=pre_tspan,\n",
    "    bounding_box=bbox,\n",
    ")\n",
    "print(\" Number of pre-event granules: \"+str(len(pre_results)))\n",
    "\n",
    "post_results = earthaccess.search_data(\n",
    "    short_name=suite_name,\n",
    "    temporal=post_tspan,\n",
    "    bounding_box=bbox,\n",
    ")\n",
    "print(\" Number of post-event granules: \"+str(len(post_results)))\n",
    "\n",
    "# in running this script in the cloud, earthaccess.open is fast, but when running locally it takes a long time (10+ min), and locally downloading the data is faster.\n",
    "# so, get the data with different approaches, depening on whether you are in the cloud or not, defined above\n",
    "if in_the_cloud == True:\n",
    "    pre_paths = earthaccess.open(pre_results, local_path=\"data\")\n",
    "    post_paths = earthaccess.open(post_results, local_path=\"data\")\n",
    "else:\n",
    "    pre_paths = earthaccess.download(pre_results, local_path=\"local_data/\")\n",
    "    post_paths = earthaccess.download(post_results, local_path=\"local_data/\")\n",
    "    print('Files Downloaded')\n",
    "\n",
    "crs, shape, transform = load_first(pre_paths[0], var_name)# once set, dont need to rerun for other vars in same suite\n",
    "pre_data = coregister_granules(pre_paths, crs, shape, transform, var_name)\n",
    "post_data = coregister_granules(post_paths, crs, shape, transform, var_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax[0].gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = pre_data.mean(\"time\").plot(x=\"longitude\", y=\"latitude\" , cbar_kwargs={'label': 'Chlor mg/m3', 'shrink':0.5}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax[0], robust=True)\n",
    "ax[0].set_xlim(bbox[0]-3,bbox[2]+3,)\n",
    "ax[0].set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax[0].coastlines()\n",
    "ax[0].set_title('pre_hurricane_8/18 & 8/19')\n",
    "\n",
    "\n",
    "ax[1].gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = post_data.mean(\"time\").plot(x=\"longitude\", y=\"latitude\", cbar_kwargs={'label': 'Chlor mg/m3', 'shrink':0.5}, cmap=cmocean.cm.haline,norm=LogNorm(vmin=.01, vmax=5),  ax=ax[1], robust=True)\n",
    "ax[1].set_xlim(bbox[0]-3,bbox[2]+3,)\n",
    "ax[1].set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax[1].coastlines()\n",
    "ax[1].set_title('post_hurricane 8/21')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the pre and post event data. first, mask both to only pixels where data is available before and after the hurricane\n",
    "# then, mask each where it's values are greater or less than\n",
    "\n",
    "pre_mean = pre_data.mean(\"time\")\n",
    "post_mean = post_data.mean(\"time\")\n",
    "\n",
    "post_mean['longitude'] = post_mean['longitude'].round(5)# round lat/lon to 5 decimals, was having issues with floats at large # of decimals not being exactly the same\n",
    "post_mean['latitude'] = post_mean['latitude'].round(5)\n",
    "pre_mean['longitude'] = pre_mean['longitude'].round(5)\n",
    "pre_mean['latitude'] = pre_mean['latitude'].round(5)\n",
    "\n",
    "# Create a mask where both datasets have valid values\n",
    "mask = ~np.isnan(post_mean) & ~np.isnan(pre_mean)\n",
    "\n",
    "# Apply mask to both datasets\n",
    "post_mean_mask = post_mean.where(mask)\n",
    "pre_mean_mask = pre_mean.where(mask)\n",
    "\n",
    "post_greater = (post_mean_mask-pre_mean_mask)/pre_mean_mask * 100\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(15, 12), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = post_greater.plot(x=\"longitude\", y=\"latitude\" , cbar_kwargs={'label': 'Percent Change', 'shrink':0.5}, cmap=cmocean.cm.balance, extend = \"neither\", ax=ax, robust=True)\n",
    "ax.set_xlim(bbox[0]-3,bbox[2]+3,)\n",
    "ax.set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax.coastlines()\n",
    "ax.set_title('Percent Change in Chlorphyll-a [%]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c71dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondary Analysis: POC. We can change the var_name variable to another parameter within the same\n",
    "# product suite, and rerun the second-half of the processing code to repeat the analysis\n",
    "\n",
    "var_name='poc'\n",
    "pre_data_poc = coregister_granules(pre_paths, crs, shape, transform, var_name)\n",
    "post_data_poc = coregister_granules(post_paths, crs, shape, transform, var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442dfa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the pre and post event data. first, mask both to only pixels where data is available before and after the hurricane\n",
    "# then, mask each where it's values are greater or less than\n",
    "\n",
    "pre_mean_poc = pre_data_poc.mean(\"time\")\n",
    "post_mean_poc = post_data_poc.mean(\"time\")\n",
    "\n",
    "post_mean_poc['longitude'] = post_mean_poc['longitude'].round(5)# round lat/lon to 5 decimals, was having issues with floats at large # of decimals not being exactly the same\n",
    "post_mean_poc['latitude'] = post_mean_poc['latitude'].round(5)\n",
    "pre_mean_poc['longitude'] = pre_mean_poc['longitude'].round(5)\n",
    "pre_mean_poc['latitude'] = pre_mean_poc['latitude'].round(5)\n",
    "\n",
    "# Create a mask where both datasets have valid values\n",
    "mask = ~np.isnan(post_mean_poc) & ~np.isnan(pre_mean_poc)\n",
    "\n",
    "# Apply mask to both datasets\n",
    "post_mean_mask_poc = post_mean_poc.where(mask)\n",
    "pre_mean_mask_poc = pre_mean_poc.where(mask)\n",
    "\n",
    "post_greater = (post_mean_mask_poc-pre_mean_mask_poc)/pre_mean_mask_poc * 100\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(15, 12), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = post_greater.plot(x=\"longitude\", y=\"latitude\" , cbar_kwargs={'label': 'Percent Change', 'shrink':0.5}, cmap=cmocean.cm.balance, extend = \"neither\", ax=ax, robust=True)\n",
    "ax.set_xlim(bbox[0]-3,bbox[2]+3,)\n",
    "ax.set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax.coastlines()\n",
    "ax.set_title('Percent Change in POC [%]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc98a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the same analysis, but for AVW\n",
    "\n",
    "pre_tspan = (\"2025-08-18\", \"2025-08-20\")    # define your pre-event and post-event timespans\n",
    "post_tspan = (\"2025-08-21\", \"2025-08-22\")\n",
    "\n",
    "min_lon = -82       # Set lat/lon extent for area of interest\n",
    "min_lat = 27.8\n",
    "max_lon = -70\n",
    "max_lat = 32.1\n",
    "\n",
    "suite_name=\"PACE_OCI_L2_AOP_NRT\" # oci suite name\n",
    "var_name = \"avw\" # variable of interest\n",
    "\n",
    "in_the_cloud = False # set to true if crunning in cloud (e.g. Cryocloud), false if running locally. For speed, granules will be downloaded to a local_data directory when set to false\n",
    "\n",
    "# do the earthdata search\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)\n",
    "if in_the_cloud == False:\n",
    "    os.makedirs('local_data/', exist_ok=True)# if not in the cloud, make a folder to downlad granules to. This folder is configured in the .gitignore to not be tracked\n",
    "\n",
    "pre_results = earthaccess.search_data(\n",
    "    short_name=suite_name,\n",
    "    temporal=pre_tspan,\n",
    "    bounding_box=bbox,\n",
    ")\n",
    "print(\" Number of pre-event granules: \"+str(len(pre_results)))\n",
    "\n",
    "post_results = earthaccess.search_data(\n",
    "    short_name=suite_name,\n",
    "    temporal=post_tspan,\n",
    "    bounding_box=bbox,\n",
    ")\n",
    "print(\" Number of post-event granules: \"+str(len(post_results)))\n",
    "\n",
    "# in running this script in the cloud, earthaccess.open is fast, but when running locally it takes a long time (10+ min), and locally downloading the data is faster.\n",
    "# so, get the data with different approaches, depening on whether you are in the cloud or not, defined above\n",
    "if in_the_cloud == True:\n",
    "    pre_paths = earthaccess.open(pre_results)\n",
    "    post_paths = earthaccess.open(post_results)\n",
    "else:\n",
    "    pre_paths = earthaccess.download(pre_results, local_path=\"local_data/\")\n",
    "    post_paths = earthaccess.download(post_results, local_path=\"local_data/\")\n",
    "    print('Files Downloaded')\n",
    "\n",
    "crs, shape, transform = load_first(pre_paths[0], var_name)# once set, dont need to rerun for other vars in same suite\n",
    "pre_data = coregister_granules(pre_paths, crs, shape, transform, var_name)\n",
    "post_data = coregister_granules(post_paths, crs, shape, transform, var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the pre and post event data. first, mask both to only pixels where data is available before and after the hurricane\n",
    "# then, mask each where it's values are greater or less than\n",
    "\n",
    "pre_mean = pre_data.mean(\"time\")\n",
    "post_mean = post_data.mean(\"time\")\n",
    "\n",
    "post_mean['longitude'] = post_mean['longitude'].round(5)# round lat/lon to 5 decimals, was having issues with floats at large # of decimals not being exactly the same\n",
    "post_mean['latitude'] = post_mean['latitude'].round(5)\n",
    "pre_mean['longitude'] = pre_mean['longitude'].round(5)\n",
    "pre_mean['latitude'] = pre_mean['latitude'].round(5)\n",
    "\n",
    "# Create a mask where both datasets have valid values\n",
    "mask = ~np.isnan(post_mean) & ~np.isnan(pre_mean)\n",
    "\n",
    "# Apply mask to both datasets\n",
    "post_mean_mask = post_mean.where(mask)\n",
    "pre_mean_mask = pre_mean.where(mask)\n",
    "\n",
    "post_greater = post_mean_mask-pre_mean_mask\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(15, 12), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.gridlines(draw_labels={\"left\": \"y\", \"bottom\": \"x\"})\n",
    "plot = post_greater.plot(x=\"longitude\", y=\"latitude\" , cbar_kwargs={'label': 'AVW Change [nm]', 'shrink':0.5}, cmap=cmocean.cm.delta, extend = \"neither\", ax=ax, robust=True)\n",
    "ax.set_xlim(bbox[0]-3,bbox[2]+3,)\n",
    "ax.set_ylim(bbox[1]-3,bbox[3]+3)\n",
    "ax.coastlines()\n",
    "ax.set_title('Change in AVW')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackweek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
